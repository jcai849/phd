Hadoop: The Definitive Guide
============================

1. Meet Hadoop
--------------

* Author argues that big data is relevant to everyone, affecting individuals
  as well as organisations; gives example of large amount of data generated
  through photos & recordings.
* The problem of big data is that storage capacities of hard drives have
  massively increased, but access speeds have not kept up.
* A potential solution is to read from multiple disks simultaneously, however
  there are drawbacks:
** Potential hardware failure
** Analysis requires combination of data between disks
* MapReduce is for batch processing, not suitable for interactive analysis
* Hadoop (being more than just MR) enables interactive analysis
* ``YARN is a cluster resource management system, which allows any distributed
    program (not just MapReduce) to run on data in a Hadoop cluster.''
* Comparison with multi-disk databases:
** B-trees as data structures form the backbone of most RDBMSs, is limited by
   seek time (seek time being the time to move the disk head to a location,
   paired with transfer rate, the disks bandwidth)
** MapReduce streams through a disk, thus limited by bandwidth
** Technologically, seek time improvement is slower than bandwidth improvement.
** Thus, RDBMSs are better for small updates, MapReduce better for full-data
   updates, and especially better for semi-structured or unstructured data
* Comparison with HPC:
** HPC approach is to distribute processing across a cluster, connected with 
   shared filesystem, using an API such as MPI for communication
** HPC works well for compute-bound jobs
** Hadoop offers data-locality, co-locating data with compute nodes, so data
   access is faster
** MPI in particular gives more programmer control, but this comes at the cost
   of manual data-flow manipulation, checkpoint, and recovery management
* History: Created by Doug Cutting, named after his child's toy elephant, used
  at Yahoo!, Facebook, etc. Broke world records in sorting big data

2. MapReduce
------------

* Running a MapReduce job with hadoop directly is a little unnerving;
  It is extremely verbose, and your eye picks up ``ERROR'' everywhere
** Mapper, Reducer, and the main classes are defined, then wrapped in .jar and
   run with hadoop. `HADOOP_CLASSPATH` should be set as the path of the .jar
* Single-machine processing limited, especially with resource management.
* ``MapReduce works by breaking the processing into two phases: the map phase
   and the reduce phase. Each phase has key-value pairs as input and output''
* Terminology:
Job:: The unit of work that the client want to be performed; it consists of the
      input data, the MapReduce program, and configuration information
Task:: Hadoop runs the job by dividing it into tasks, of which there are two
       types: map tasks and resuce tasks. The tasks are scheduled using YARN
       and run on nodes in the cluster. If a task fails, it will be
       automatically rescheduled to run on a different node.
Split:: Hadoop divides the input to a MapReduce job into fixed-size pieces
	called input splits, or just splits. Hadoop creates one map task for
	each split, which runs the user-defined map function for each record in
	the split
* Load balancing is better when splits are smaller, to allow faster machines to
  take more splits. However, if splits are too small, too much processing goes
  into the overhead of managing splits. The recommended split size tends to be
  the size of an HDFS block (128Mb by default).
Data locality optimisation:: Hadoop tries to run the map task on a node where
	the input data resides in HDFS, to not use cluster bandwidth.
* Map tasks write their output to the local disk, not to HDFS, as they are only
  meant to be temporary results
Combiner:: Combiner functions can be defined which run on the map output at the
	   location of the mapper, serving to reduce the information sent
	   across the cluster to reducers, saving bandwidth.
* Hadoop can make use of arbitrary programs to serve as mappers and reducers,
  through streaming to stdin and stdout of the programs; this allows arbitrary
  languages to run MapReduce.

3. The Hadoop Distributed Filesystem
------------------------------------

.Setting up a Pseudo-Distributed Cluster
. A stable version of Hadoop to be downloaded (better to just use the binaries)
  Older versions are more stable, as well as better supported by spark and 3rd
  party documentation (you're going to need it)
. Set up environment variable HADOOP_HOME, and add the bin and sbin folders to
  PATH
. Configuration files are essential; 
.. Setup a configuration folder (or use the etc directory in HADOOP_HOME;
   external folder is better) and point environment variable HADOOP_CONF_DIR
   to the folder
.. Fill it with config files: I used the files from
   https://github.com/tomwhite/hadoop-book/tree/master/conf[the book's GitHub
   repo]. Amusingly, the ``definitive guide'' required more than what was 
   provided, including a `capacity-scheduler.xml` file, from
   http://svn.apache.org/viewvc/hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/resources/capacity-scheduler.xml?revision=1495684
.. Add localhost to the slave file for YARN to setup the ResourceManager
. Now the cluster can be started. Format the namenode with 
+
[source,sh]
--------------------
hdfs namenode -format
---------------------
. Start the HDFS, YARN, and MapReduce daemons (in that order) with
+
[source,sh]
----------------------------------------
start-dfs.sh
start-yarn.sh
mr-jobhistory-daemon start historyserver
----------------------------------------
+
The effects can be checked through several ways:

.. Checking the logs in the $HADOOP_HOME/logs directory
.. `jps` to see what Java processes are running
.. `hdfs getconf -namenodes` and co. for direct queries
.. Web interface with the namenode on port 50070
. If that's all working, make a user directory with
+
[source,sh]
-------------------------------
hadoop fs -mkdir -p /user/$USER
-------------------------------
+
the command has a few other standard unix navigation utilities. A `tail` but
no `head` though, due to the HDFS block structure!

. Copy the local files of interest over with
+
[source,sh]
--------------------------------------------------------------------------------
hadoop fs -copyFromLocal <local> hdfs://localhost/user/$USER/<intended-location>
--------------------------------------------------------------------------------
+
Worth noting that hdfs is the default, and relative paths can be given

. Run the MapReduce job in the standard manner on the HDFS files; see
  $HADOOP_HOME/log if it doesn't work

. Stop the daemons in reverse order with
+
[source,sh]
----------------------------------------
mr-jobhistory-daemon stop historyserver
stop-yarn.sh
stop-dfs.sh
----------------------------------------

4. YARN
-------

5. Hadoop I/O
-------------

10. Setting Up a Hadoop Cluster
-------------------------------

11. Administering Hadoop
------------------------

17. Hive
--------

19. Spark
---------

A. Installing Apache Hadoop
---------------------------

