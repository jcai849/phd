\documentclass[10pt,a4paper]{article}

\usepackage{hyperref}
\usepackage{biblatex}
\addbibresource{bib/bibliography.bib}
\usepackage{csquotes}

\begin{document}

\title{Distributed Systems Overview}
\author{Jason Cairns}
\maketitle{}

\tableofcontents{}

\section{Hadoop}
\label{sec:hadoop-1}

Apache Hadoop is a collection of utilities that facilitates cluster
computing. Jobs can be sent for parallel processing on the cluster
directly to the utilities using .jar files, ``streamed'' using any
executable file, or accessed through language-specific APIs.

The project began in 2006, by Doug Cutting, a Yahoo employee, and Mike
Cafarella. The inspiration for the project was a paper from Google
describing the Google File System (described in
\textcite{ghemawat2003google}), which was followed by another Google
paper detailing the MapReduce programming model,
\textcite{dean2004mapreduce}.

Hadoop consists of a memory part, known as Hadoop Distributed File
System (HDFS), described in Section~\ref{sec:hdfs}, and a processing part,
known as MapReduce, described in Section~\ref{sec:mapreduce}.

In operation, Hadoop splits files into blocks, then distributes them
across nodes in a cluster, where they are then processed by the node.
This creates the advantage of data locality, wherein data is processed
by the node they exist in.

Hadoop has seen extensive industrial use as the premier big data
platform upon it's release. In recent years it has been overshadowed
by Spark, due to the greater speed gains offered by spark. The key
reason Spark is so much faster than Hadoop comes down to their
different processing approaches: Hadoop MapReduce requires reading
from disk and writing to it, for the purposes of fault-tolerance,
while Spark can run processing entirely in-memory. However, in-memory
MapReduce is provided by another Apache project,
Ignite\cite{zheludkov2017high}.


\subsection{Hadoop Distributed File System}
\label{sec:hdfs}

The file system has 5 primary services.

\begin{description}
\item[Name Node] Contains all of the data and manages the system. The
  master node.
  \item[Secondary Name Node] Creates checkpoints of the metadata from
  the main name node, to potentially restart the single point of
  failure that is the name node. Not the same as a backup, as it only
  stores metadata.
\item[Data Node] Contains the blocks of data. Sends ``Heartbeat
  Message'' to the name node every 3 seconds. If two minutes passes
  with no heartbeat message from a particular data node, the name node
  marks it as dead, and sends it's blocks to another data node.
\item[Job Tracker] Receives requests for MapReduce from the client,
  then queries the name node for locations of the data.
\item[Task Tracker] Takes tasks, code, and locations from the job
  tracker, then applies such code at the location. The slave node for
  the job tracker.
\end{description}

HDFS is written in Java and C. It is described in more detail in
\textcite{shvachko2010hadoop}


\subsection{MapReduce}
\label{sec:mapreduce}

MapReduce is a programming model consisting of map and reduce staps,
alongside making use of keys.

\begin{description}
\item[Map] applies a ``map'' function to a dataset, in the
  mathematical sense of the word. The output data is temporarily
  stored before being shuffled based on output key, and sent to the
  reduce step.
\item[Reduce] produces a summary of the dataset yielded by the map operation
\item[Keys] are associated with the data at both steps. Prior to the
  application of mapping, the data is sorted and distributed among
  data nodes by the data's associated keys, with each key being mapped
  as a logical unit. Likewise, mapping produces output keys for the
  mapped data, and the data is shuffled based upon these keys, before
  being reduced.
\end{description}

After sorting, mapping, shuffling, and reducing, the output is
collected, sorting by the second keys and given as final output.

The implementation of MapReduce is provided by the HDFS services of
job tracker and task tracker. The actual processing is performed by
the task trackers, with scheduling using the job tracker, but other
scheduling systems are available to be made use of.

Development at Google no longer makes as much use of MapReduce as they
originally did, using stream processing technologies such as
MillWheel, rather than the standard batch processing enabled by
MapReduce\cite{akidau2013millwheel}.

\section{Spark}
\label{sec:spark}

Spark is a framework for cluster computing\cite{zaharia2010spark}. Much of it's definition is
in relation to Hadoop, which it intended to improve upon in terms of
speed and usability for certain tasks.

It's fundamental operating concept is the Resiliant Distributed
Dataset (RDD), which is immutable, and generated through external
data, as well as actions and transformations on prior RDD's. The RDD
interface is exposed through an API in various languages, including R.

Spark requires a distributed storage system, as well as a cluster
manager; both can be provided by Hadoop, among others.

Spark is known for possessing a fairly user-friendly API, intended to
improve upon the MapReduce interface. Another major selling point for
Spark is the libraries available that have pre-made functions for
RDD's, including many iterative algorithms. The availability of
broadcast variables and accumulators allow for custom iterative
programming.

Spark has seen major use since it's introduction, with effectively all
major big data companies having some use of Spark. It's features and
implementations are outlined in \textcite{zaharia2016apache}.
 

\section{H2O}
\label{sec:h2o}

The H2O software bills itself as,

\begin{displaycquote}{h2o.ai:_home_open_sourc_leader_ai}
  an in-memory platform for distributed, scalable machine learning.
  H2O uses familiar interfaces like R, Python, Scala, Java, JSON and
  the Flow notebook/web interface, and works seamlessly with big data
  technologies like Hadoop and Spark. H2O provides implementations of
  many popular algorithms such as GBM, Random Forest, Deep Neural
  Networks, Word2Vec and Stacked Ensembles. H2O is extensible so that
  developers can add data transformations and custom algorithms of
  their choice and access them through all of those clients.
\end{displaycquote}

H2O typically runs on HDFS, along with spark for computation and
bespoke data structures serving as the backbone of the architecture.

H2O can communicate with R through a REST api. Users write functions
in R, passing user-made functions to be applied on the objects
existing in the H2O system\cite{h2o.ai:_h2o}.

The company H2O is backed by \$146M in funding, partnering with large
institutions in the financial and tech world. Their business model
follows an open source offering with the same moniker as the company,
and a small set of heavily-marketed proprietary software in aid of it.
They have some important figures working with them, such as Matt
Dowle, creator of data.table.

\section{R Packages}
\label{sec:r-packages}

\subsection{DistributedR}
\label{sec:distributedr}
DistributedR offers cluster access for various R data structures,
particularly arrays, and providing S3 methods for a fair range of
standard functions. It has no regular cluster access interface, such
as with Hadoop or MPI, being made largely from scratch.

The package creators have ceased development as of December 2015. The
company, Vertica, has moved on to offering an enterprise database
platform\cite{vertica:_distr}.

\subsection{Foreach and DoX}
\label{sec:foreach-dox}

Foreach offers a high-level looping construct compatible with a
variety of backends\cite{microsoft20}. The backends are provided by
other packages, typically named with some form of ``Do\textit{X}''.
Parallelisation is enabled by some backends, with doParallel allowing
parallel computations\cite{corporation19}, doSNOW enabling cluster
access through the SNOW package\cite{dosnow19}, and doMPI allowing for
direct MPI access\cite{weston17}.

Foreach is managed by Revolution Analytics, with many of the
Do\textit{X} corollary packages also being produced by them. Further
information of foreach is given in \textcite{weston19:_using}.

\subsection{Future}
\label{sec:future-furrr}

Future captures R expressions for evaluation, allowing them to be
passed on for parallel and ad-hoc cluster evaluation, through the
parallel package\cite{bengtsson20}. Such parallelisation uses the
standard MPI or SOCK protocols.

The author of future is Henrik Bengtsson, Associate Professor at UCSF.
Development on the package remains strong, with Dr.~Bengtsson
possessing a completely full commit calendar and 81,328 contributions
on GitHub. I have written more on future in the document,
``more-on-future.tex''. Future has many aspects to it, captured in
it's extensive series of
vignettes\cite{bengtsson20:_futur_r}\cite{bengtsson20:_futur_r2}\cite{bengtsson20:_futur_r3}\cite{bengtsson20:_futur_r4}\cite{bengtsson20:_futur_r5}\cite{bengtsson20:_futur_r6}.

Furrr is a frontend to future, amending the functions from the package
purrr to be compatible with future, thus enabling parallelisation in a
similar form to multicore, though with a tidyverse
style\cite{vaughan18}.

Furrr is developed by Matt Dancho, and Davis Vaughn, an employee at
RStudio.

\subsection{Parallel, Snow, and Multicore}
\label{sec:parall-snow-mult}
Parallel is a package included with R, born from the merge of the
packages snow and multicore\cite{core:_packag}. Parallel enables
various means of performing computations in R in parallel, allowing
not only multiple cores in a node, but multiple nodes through snow's
interfaces to MPI and SOCK\cite{tierney18}.

Parallel takes from multicore the ability to perform multicore
processing, with the mcapply function. Multicore creates forked R
sessions, which is very resource-efficient, but not supported by
windows.

From snow, distributed computing is enabled for multiple nodes.

Multicore was developed by Simon Urbanek (!). Snow was developed by
Luke Tierney, a professor at the University of Iowa, who also
originated the byte-compiler for R

\subsection{Partools}
\label{sec:partools}

Partools provide utilities for the parallel
package\cite{matloff16:_softw_alchem}. It offers functions to split
files and process the splits across nodes provided by parallel, along
with bespoke statistical functions.

It consists mainly of wrapper functions, designed to follow it's
philosophy of ``keep it distributed''.

It is authored by Norm Matloff, a professor at UC, Davis.

\subsection{PBDR}
\label{sec:pbdr}


pbdR is a collection of packages allowing for distributed computing
with R\cite{pbdBASEpackage}. The packages include communication and
computation capabilities, including RPC, ZeroMQ, and MPI interfaces.
More detail is given in \textcite{pbdBASEvignette}

\subsection{RHadoop}
\label{sec:rhadoop}

RHadoop is a collection of five packages to run Hadoop directly from
R\textcite{analytics:_rhadoop_wiki}. The packages are divided by
logical function, including rmr2, which runs MapReduce jobs, and
rhdfs, which can access the HDFS. The packages also include plyrmr,
which makes available plyr-like data manipulation functions, in a
similar vein to sparklyr.

It is offered and developed by Revolution Analytics.
  
\subsection{RHIPE and DeltaRho}
\label{sec:rhipe-deltarho}

RHIPE is a means of ``using Hadoop from R''\cite{deltarho:_rhipe}. The
provided functions primarily attain this through interfacing and
manipulating HDFS, with a function, rhwatch, to submit MapReduce jobs.
The easiest means of setup for it is to use a VM, and for all Hadoop
computation, MapReduce is directly programmed for by the user.

There is currently no support for the most recent version of Hadoop,
and it doesn't appear to be under active open development, with the
last commit being 2015. RHIPE has mostly been subsumed into the
backend of DeltaRho, a simple frontend.
  
\subsection{Rmpi}
\label{sec:rmpi}

Rmpi is an R interface for MPI\cite{yu02:_rmpi}. As such, the
capabilities enabled by the package are best suited for
supercomputers, such as Beowulf clusters or Cray.

It is authored by Hao Yu, of the University of Western Ontario.

\subsection{Sparklyr}
\label{sec:sparklyr}

Sparklyr is an interface to Spark from within R\cite{luraschi20}. The user connects to
spark and accumulates instructions for the manipulation of a Spark
DataFrame object using dplyr commands, then executing the request on
the Spark cluster.

Sparklyr is managed and maintained by RStudio, who also manage the
rest of the Tidyverse (including dplyr).

\subsection{SparkR}
\label{sec:sparkr}

SparkR provides a front-end to Spark from
R\cite{venkataraman20:_spark}. Like Sparklyr, it provides the
DataFrame as the primary object of interest. However, there is no
support for the dplyr model of programming, with functions closer
resembling base R being provided by the package instead.

SparkR is maintained directly by Apache Spark, with ongoing regular
maintenance provided. Usage of the package is described in the
vignette, \textcite{venktaraman19:_spark_pract_guide}, with
implementation explained in \textcite{venkataraman2016sparkr}.

\subsection{Hmr}
\label{sec:hmr}

Hmr is an interface to MapReduce from R\cite{urbanek20}. It runs super
fast, making use of chunked data. Much of the process is handled by
the package, with automatic R object conversion. Hmr integrates with
iotools, of which it is based upon. The author, like that of iotools,
is Simon Urbanek.

\subsection{Big.data.table}
\label{sec:big.data.table}

Big.data.table runs data.table over many nodes in an ad-hoc
cluster\cite{gorecki16}. This allows for big data manipulation using a
data.table interface. The package makes use of Rserve (authored by
Simon Urbanek) to facilitate communication between nodes when running
from R. Alternatively, the nodes can be run as docker services, for
fast remote environment setup, using RSclient for connections. Beyond
greater storage capacity, speed is increased through manipulations on
big.data.tables occurring in parallel. The package is authored by Jan
Gorecki, but hasn't been actively developed since mid-2016.

\section{R Derivatives}
\label{sec:r-derivatives}

\subsection{ML Services on HDInight}
\label{sec:r-server-hdinsight}

ML Services on HDInsight, also referred to as R Server, is a
distribution of R with particular emphasis on parallel and
capabilities\cite{azure16:_r_server_hdins_r_analy}. Specific parallel
capabilities advertised include multi-threaded mathematics libraries.
R Server for HDInsight was previously named Revolution R, and
developed by Revolution Analytics, before being bought out by
Microsoft, renamed Microsoft R, before acquiring the name R server for
HDInsight, before changing to ML Services.

\subsection{IBM Big R}
\label{sec:ibm-big-r}

IBM Big R is a library of functions integrating R with the IBM
InfoSphere BigInsights platform\cite{inc.14:_infos_bigin_big_r}. This
is implemented through the introduction of various bigr classes
replicating base R types. These are then run in the background on the
BigInsights platform, which is in turn powered by Apache Hadoop. The
user is therefore able to run MapReduce jobs without having to
explicitly write MapReduce-specific code.

\subsection{MapR}
\label{sec:mapr}

MapR initially provided R access to Hadoop, being mainly HDFS
access\cite{mapr19:_indus_next_gener_data_platf_ai_analy}. It was then
bought out by HP in May 2019, pivoting to selling an enterprise
database platform and analytics services, running on Hadoop among
other backends. Development has ceased on R access.


\printbibliography{}
\end{document}
