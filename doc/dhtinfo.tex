\documentclass[10pt, a4paper]{article}
\usepackage{header}
\begin{document}
\title{Distributed Hash Table for the Dependency Graph}
\year=2021 \month=8 \day=23
\maketitle

\section{Introduction}

The use of an intermediary message broker for RPC's within the system has proved exceptionally useful.
Currently, rather than sending messages directly from node-to-node, all messages are sent to queues named after chunks on a central Redis server, which routes the messages to nodes holding those chunks and performing blocking pops on the queues.
The layer of indirection provided by this setup has enabled information hiding effectively in support of system modularity, as well as a fair degree of dynamism in chunk location.

The principal issue with this approach is the high level of centralisation it entails.
Such centralisation leads to casting the Redis server as a single point of failure, and requiring inherent knowledge of the location of the Redis server for connecting nodes.
This is largely offset by the high efficiency and low complexity afforded by the approach, but it is worth considering a more dynamic alternative in the implementation of a Distributed Hash Table (DHT) as a supporting transport layer for the system.

This document seeks to outline the motivation, implementation, and some variations of a DHT as part of the \lsr{} system.

A key motivator for the usage of a DHT transport layer, rather than a central Redis server, beyond the independence and decentralisation engendered, is also in enabling a distributed store of the dependency graph, as outlined in XXX.
The advantages of a DHT in this respect include the ability to scale with the system, through CPU and memory load being spread through the system, as well as greater live fault-tolerance via in-memory replication.
Perhaps even more pressingly, a DHT algorithm as implemented in the system affords more control over components such as the potential for callbacks and hooks, without having to involve Redis-specific Lua API.
This means that there is greater direct control over the graph by nodes holding each partition of it, which has shown to be a shortcoming of Python's Dask; Dask has an explicit dependency graph, contained entirely within the master node, which leaves it out of reach of nodes that it affects directly.

\section{Overview of DHT's}

A DHT is a key-value store with the capacity to be distributed over many disparate nodes, and all modern DHT's involve no central control, with all participating nodes being effectively equivalent and symmetrical.

Distributed Hash Table algorithm generally share a few features in common, beyond those inherent in the DHT name:
\begin{itemize}
	\item Dynamic introduction and exit of nodes is a principal distinguishing feature of DHT's with respect to regular hash tables using nodes as buckets.
		Resizing of naive hash tables often involves a complete rehashing and remapping of values to the buckets, whereas DHT's expect frequent resizing, and can't afford the remapping.
		DHT's then have some hashing function that doesn't require system-wide remapping when nodes/buckets are added or removed.
	\item Nodes typically have some node ID that of the same length as keys (or some variation on keys, such as their SHA-1 hash), in order to allow for some direct comparison between them. Keys are usually hashed to particular nodes based on some measure of distance, choosing the node with the lowest distance to store the value on.
	\item Minimisation of route length and minimisation of degree are principal objectives in the analysis of DHT's, with most DHT's sporting \bigO{\log n} lookup time in the worst case.
	\item Susceptibility to Byzantine faults such as the Sybil attack; given that the \lsr{} system is expected to run in a non-adverserial environment, this consideration doesn't factor too heavily into choice of DHT algorithm.
\end{itemize}

Significant work on DHT's occured within a few years following the year 2000, with very little major innovation occuring before or since.
The primary algorithms introduced include Chord and Kademlia, alongside others such as Pastry, Tapestry, and Koorde.
Chord is among the more simple of DHT's, with Kademlia possessing some advantages in payment for some additional complexity.
The following subsections describe the algorithms, including reasons for choosing Kademlia as the algorithm for the DHT message transport layer in \lsr{}, as well as some potential drawbacks.

\subsection{Chord}

Chord builds on the consistent hashing algorithm, adding linear lookup and finger tables.

Consistent hashing is a hashing scheme with a high probability of load balancing and minimisation of key remapping upon node exit and joining of the network, with only \bigO{\frac{1}{N}} expected key movement.
Consistent hashing relies upon an identifier circle, as a form of modulo ring.
The identifier ring exists conceptually at the size $2^m$, $m$ simply being large enough to make the probability of hash collision negligable.
Each node is hashed to the identifier ring based upon it's node ID, typically with the SHA-1 hash.
Keys are then assigned to nodes by determining their to point on the ring, using the same hash function used for nodes, and specifying that their node assignment is to be the first node clockwise on the ring following that key, with that node termed as the key's successor.

Based on the description of key-node assignments given by consistent hashing, the Chord algorithm allows decentralised coordination over the ring through provision of the single lookup ability, which can be used to return either a node or a key.
The central requirement is that each node knows it's successor.
With this in place, finding a node, or a key, involves recursively successors, with successors forwarding the query to their own successors in a recursive manner, until the node or value is found.
As this is \bigO{N} in terms of route length, an additional stipulation is given that nodes carry a finger table, wherein addresses of nodes in exponentially increasing intervals are stored, and finger tables are consulted for routing instead of bare successors.
The nodes in the finger table are the successors to the points at increasing values of $2^k, 1 \leq k \leq m$.
Successors to points are found based on querying the maximal node less than the point in the existing finger table, and recursively passing the query, until a queried node finds that the queried point lies between itself and it's own successor, at which point it returns it's own successor.

Nodes join the network by finding their successor through querying any arbitrary node in the network.
The arrival of new nodes has the potential to throw off existing finger tables, and as such a stabilisation routine must be run periodically to maintain the system, by checking consistency of successors and predecessors.
The need for a regular (hourly in the original paper) stabilisation routine is a source of lookup failure, and is not aided by the fact that the distance measure is asymmetric, so new node information is not transmitted through the network without the periodic stabilisation routines.
This drawback is significant and cause for pursuing a more complex algorithm in the hope of gaining more guarantees.

\subsection{Kademlia}

Kademlia is a more complex algorithm than Chord, though possesses certain features that make it more amenable to a truly dynamic distributed system.
Kademlia sees use by the Ethereum cryptocurrency, the Tox messaging protocol, and the gnutella file sharing system.
Also has potential for lookup failure if newly added nodes are closer to key, and keyholder not updated accordingly

\section{Alternatives to Standard DHT Approaches}

non-internet scale network so could just maintain table of all nodes - \bigO{N} routing table not bad ifN < 1000, and will aid in routing. mesh? skipgraph?
skipgraph;
redis uses skiplist for ordered sets: % https://github.com/redis/redis/blob/unstable/src/t_zset.c
Mesh DHT

\section{Value and Key Descriptions}

\section{Relation to System}
bilayer

Return address or values?

\section{Variations}
avoiding republishing by relaying new node joins around the network
replication/caching? not yet
master must have invalid node id; add -NONPARTICIPANT flag to RPC's

\bib{bibliography}
\end{document}
