So there is more data, more data storage, far more powerful processing ability, and all are on an exponential path of growth, with standardly available RAM remaining far from capable of servicing the deluge of data made possible by all of this growth\cite{sutter2005free}.
The problem of larger-than-memory datasets is certainly not going away.

To put the growth of data storage capacity in perspective, by one measure information storage capacity has grown at a compound annual rate of 23\% per capita over recent decades\cite{hilbert2011world}.
In spite of such massive growths in storage capacity, they are far outstripped by computational capacity over time \cite{fontana2018moore}.
Specifically, the number of components comprising an integrated circuit for computer processing have been exponentially increasing, with an additional exponential decrease in their cost\cite{moore1975progress}.
This observation, known as Moore's Law, has been the root cause for much of computational advancement over the past half-century.
The corresponding law for computer storage posits increase in bit density of storage media along with corresponding decreases in price, which has been found to track lower than expected by Moore's law metrics\cite{hilbert2011world}.
In more recent years, the use of these additional transistors in the CPU has been focussed not so much on increased clock speed as was the case pre-2003, but the focus has shifted to multicore processing, for speed has met it's limiting factors in the heat produced and power consumed\cite{sutter2005free}.

Software techniques are the best solution to the problem that hardware enables, as there is no alternative shy of investing in supercomputers, something far out of reach of most individuals and organisations.
The software solution includes the programming in the little, to take advantage of greater hardware support for concurrency, as well as the design of systems that are architected in such a fashion as to surmount constraints in hardware capacity.
This project puts greater focus on the latter.

The study of systems capable of handling larger-than-memory data extends back decades, and spans a very broad literature.
Of utmost relevance to this project are modern systems with statistical capabilities.
The approaches can be roughly divided into two main categories: local, and distributed.
Local systems make of a single computer for data processing, taking advantage of the larger data storage capacity of disk, and often making use of parallel processing.

Distributed systems spread the data load across multiple computers.
Though more complex, they tend to be faster and more capable when working with larger data, due to the greater parallelism afforded, as well as the larger pool of main memory available.

A common local solution is to just treat disk memory as an extension of RAM, as working memory.
Some systems do make use of this approach, and will be examined below, but this is a non-starter for truly large data, as it is orders of magnitude slower than if the dataset were being held in RAM.
The key to a distributed approach, as well as some aspects of local approaches, is taking advantage of parallelisation, for both processing and memory advantages.

Parallelisation is a well-trod field, despite the multicore offering in consumer computers being a relatively recent trend.
To parallelise is to engage in many computations simultaneously - this typically takes the form of either task parallelism, wherein tasks are distributed across different processors; data parallelism, where the same task operates on different pieces of data across different processors; or some mix of the two.
Parallelism can afford major speedups, albeit with certain limitations.
Amdahl's law, formulated in 1967, aims to capture the speedup limitations, with a model derived from the argument given below in formula \ref{amdahlsform}\cite{amdahl1967law}\cite{gustafson1988law}:

\begin{equation}
        \label{amdahlsform}
        \textrm{Speedup} = \frac{1}{s+\frac{p}{N}}
\end{equation}

Where,

\begin{itemize}
        \item \(Speedup\) total speedup of whole task
        \item \(s\) time spend by serial processor on inherently serial part of program
        \item \(p\) time spent by serial processor on parallelisable part of program
        \item \(N\) number of processors
\end{itemize}

The implication is that speedup of an entire task when parallelised is granted only through the portion of the task that is otherwise constrained by singular system resources, at the proportion of execution time spent in that task.
Thus a measure of skepticism is contained in Amdahl's argument, with many tasks predicted to show no benefit to parallelise - and in reality, some likely to slow down with increased overhead given in parallelisation.
The major response to the skepticism of Amdahl's law is given by Gustafson's law, generated from timing results in a highly parallelised system.
Gustafson's law presents a scaled speedup as per equation \ref{gustafsonsform}

\begin{equation}
        \label{gustafsonsform}
        \textrm{Scaled speedup} = s' + p'N = N + (1-N)s'
\end{equation}

Where,

\begin{itemize}
        \item \(s'\) serial time spent on the parallel system
        \item \(p'\) parallel time spent on the parallel system
\end{itemize}

This law implies far higher potential parallel speedup, varying linearly with the number of processors.

An example of an ideal task for parallelisation is the category of embarassingly parallel workload.
Such a problem is one where the separation into parallel tasks is trivial, such as performing the same operation over a dataset independently\cite{foster1995parallel}.
Many problems in statistics fall into this category, such as tabulation, monte-carlo simulation and many matrix manipulation tasks.

When applied to the distributed world, some additional terminology is made use of.
In networks, individual computers are referred to as \textit{nodes}, and a distributed system will be said to take advantage of some number of nodes.
The pieces of data that are split up and distributed over the nodes are referred to as \textit{chunks} or \textit{shards}, among a variety of other names, though \textit{chunks} are the term made use of in this and successive documents.
Finally, a programming reference to these chunks can take the form of a \textit{distributed object}, which serves as an object-oriented interface to enable data manipulation via chunk manipulation at varying degrees of transparency.

With this in place, some of the main approaches to handling large-scale data can be explored in detail.

\subsection{Local Systems}

In the space of local solutions, offloading additional data to disk is the central solution.
This is best illustrated through the \textbf{disk.frame} package, developed by Dai ZJ.
\textbf{disk.frame} provides an eponymously named dataframe replacement class, which is able to represent a data  set far larger than RAM, constrained now only by disk size\cite{zj20}.
It's mechanism of action is to use chunks of data on disk, and provide a variety of methods taking advantage of data.frame generics, including \textbf{dplyr} and \textbf{data.table} functions.
disk.frames are actually references to compressed files in a filesystem directory on disk, with each file serving as a chunk.
Operation is given through manipulation of each chunk separately, loading a constrained number of chunks into RAM at a time, sparing the computer from dealing with a single monolithic file\cite{zj19:_inges_data}.
As mentioned above, this breaks down at scale, with the transfer of data from disk to RAM and back being far too slow for anything particularly big.

The chunking and loading strategy also finds it's way into statistical models.
\textbf{disk.frame} also offers an aspect of this, with linear modelling and generalised linear modelling functions calling the \textbf{biglm} package, which builds models a chunk at a time.

It is also worth bearing in mind that parallelism manifests itself within the single computer and works well for chunk processing.
A number of \textbf{R} packages, \textbf{disk.frame} included, take advantage of various parallel strategies in order to process large datasets efficiently.
\textbf{multicore} is one such package, now subsumed into the \textbf{parallel} package, that grants functions that can make direct use of multiprocessor systems, thereby reducing the processing time in proportionality to the number of processors available on the system.

\subsection{Distributed Systems}

At some stage however, the data gets too big for one computer.
When this is the case, a variety of distributed systems approaches have been put forward.

\subsection{Evaluation}
