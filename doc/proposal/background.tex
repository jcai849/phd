So there is more data, more data storage, far more powerful processing ability, and all are on an exponential path of growth, with standardly available RAM remaining far from capable of servicing the deluge of data made possible by all of this growth\cite{sutter2005free}.
The problem of larger-than-memory datasets is certainly not going away.

To put the growth of data storage capacity in perspective, by one measure information storage capacity has grown at a compound annual rate of 23\% per capita over recent decades\cite{hilbert2011world}.
In spite of such massive growths in storage capacity, they are far outstripped by computational capacity over time \cite{fontana2018moore}.
Specifically, the number of components comprising an integrated circuit for computer processing have been exponentially increasing, with an additional exponential decrease in their cost\cite{moore1975progress}.
This observation, known as Moore's Law, has been the root cause for much of computational advancement over the past half-century.
The corresponding law for computer storage posits increase in bit density of storage media along with corresponding decreases in price, which has been found to track lower than expected by Moore's law metrics\cite{hilbert2011world}.
In more recent years, the use of these additional transistors in the CPU has been focussed not so much on increased clock speed as was the case pre-2003, but the focus has shifted to multicore processing, for speed has met it's limiting factors in the heat produced and power consumed\cite{sutter2005free}.

Software techniques are the best solution to the problem that hardware enables, as there is no alternative shy of investing in supercomputers, something far out of reach of most individuals and organisations.
The software solution includes the programming in the little, to take advantage of greater hardware support for concurrency, as well as the design of systems that are architected in such a fashion as to surmount constraints in hardware capacity.
This project puts greater focus on the latter.

The study of systems capable of handling larger-than-memory data extends back decades, and spans a very broad literature.
Of utmost relevance to this project are modern systems with statistical capabilities.
The approaches can be roughly divided into two main categories: local, and distributed.
Local systems make of a single computer for data processing, taking advantage of the larger data storage capacity of disk, and often making use of parallel processing.

Distributed systems spread the data load across multiple computers.
Though more complex, they tend to be faster and more capable when working with larger data, due to the greater parallelism afforded, as well as the larger pool of main memory available.

A common local solution is to just treat disk memory as an extension of RAM, as working memory.
Some systems do make use of this approach, and will be examined below, but this is a non-starter for truly large data, as it is orders of magnitude slower than if the dataset were being held in RAM.
The key to a distributed approach, as well as some aspects of local approaches, is taking advantage of parallelisation, for both processing and memory advantages.

Parallelisation is a well-trod field, despite the multicore offering in consumer computers being a relatively recent trend.
To parallelise is to engage in many computations simultaneously - this typically takes the form of either task parallelism, wherein tasks are distributed across different processors; data parallelism, where the same task operates on different pieces of data across different processors; or some mix of the two.
Parallelism can afford major speedups, albeit with certain limitations.
Amdahl's law, formulated in 1967, aims to capture the speedup limitations, with a model derived from the argument given below in formula \ref{amdahlsform}\cite{amdahl1967law}\cite{gustafson1988law}:

\begin{equation}
        \label{amdahlsform}
        \textrm{Speedup} = \frac{1}{s+\frac{p}{N}}
\end{equation}

Where,

\begin{itemize}
        \item \(Speedup\) total speedup of whole task
        \item \(s\) time spend by serial processor on inherently serial part of program
        \item \(p\) time spent by serial processor on parallelisable part of program
        \item \(N\) number of processors
\end{itemize}

The implication is that speedup of an entire task when parallelised is granted only through the portion of the task that is otherwise constrained by singular system resources, at the proportion of execution time spent in that task.
Thus a measure of skepticism is contained in Amdahl's argument, with many tasks predicted to show no benefit to parallelise - and in reality, some likely to slow down with increased overhead given in parallelisation.
The major response to the skepticism of Amdahl's law is given by Gustafson's law, generated from timing results in a highly parallelised system.
Gustafson's law presents a scaled speedup as per equation \ref{gustafsonsform}

\begin{equation}
        \label{gustafsonsform}
        \textrm{Scaled speedup} = s' + p'N = N + (1-N)s'
\end{equation}

Where,

\begin{itemize}
        \item \(s'\) serial time spent on the parallel system
        \item \(p'\) parallel time spent on the parallel system
\end{itemize}

This law implies far higher potential parallel speedup, varying linearly with the number of processors.

An example of an ideal task for parallelisation is the category of embarassingly parallel workload.
Such a problem is one where the separation into parallel tasks is trivial, such as performing the same operation over a dataset independently\cite{foster1995parallel}.
Many problems in statistics fall into this category, such as tabulation, monte-carlo simulation and many matrix manipulation tasks.

When applied to the distributed world, some additional terminology is made use of.
In networks, individual computers are referred to as \textit{nodes}, and a distributed system will be said to take advantage of some number of nodes.
The pieces of data that are split up and distributed over the nodes are referred to as \textit{chunks} or \textit{shards}, among a variety of other names, though \textit{chunks} are the term made use of in this and successive documents.
Finally, a programming reference to these chunks can take the form of a \textit{distributed object}, which serves as an object-oriented interface to enable data manipulation via chunk manipulation at varying degrees of transparency.

With this in place, some of the main approaches to handling large-scale data can be explored in detail.

\subsection{Local Systems}

In the space of local solutions, offloading additional data to disk is the central solution.
This is best illustrated through the \textbf{disk.frame} package, developed by Dai ZJ.
\textbf{disk.frame} provides an eponymously named dataframe replacement class, which is able to represent a data  set far larger than RAM, constrained now only by disk size\cite{zj20}.
It's mechanism of action is to use chunks of data on disk, and provide a variety of methods taking advantage of data.frame generics, including \textbf{dplyr} and \textbf{data.table} functions.
disk.frames are actually references to compressed files in a filesystem directory on disk, with each file serving as a chunk.
Operation is given through manipulation of each chunk separately, loading a constrained number of chunks into RAM at a time, sparing the computer from dealing with a single monolithic file\cite{zj19:_inges_data}.
As mentioned above, this breaks down at scale, with the transfer of data from disk to RAM and back being far too slow for anything particularly big.

The chunking and loading strategy also finds it's way into statistical models.
\textbf{disk.frame} also offers an aspect of this, with linear modelling and generalised linear modelling functions calling the \textbf{biglm} package, which builds models a chunk at a time.

It is also worth bearing in mind that parallelism manifests itself within the single computer and works well for chunk processing.
A number of \textbf{R} packages, \textbf{disk.frame} included, take advantage of various parallel strategies in order to process large datasets efficiently.
\textbf{multicore} is one such package, now subsumed into the \textbf{parallel} package, that grants functions that can make direct use of multiprocessor systems, thereby reducing the processing time in proportionality to the number of processors available on the system.

\subsection{Distributed Systems}

At some stage however, the data gets too big for one computer.
When this is the case, a variety of distributed system approaches have been put forward.

This section of the field is heavily dominated by major existing systems outside of R, so most current approaches serve as basic interfaces to the external system, complete with all of the expected abstraction leaks\cite{spolsky2002abstraction}.

Of the few systems which are unique to \textbf{R}, the \textbf{SNOW} (Simple Network Of Workstations) package stands out composing part of the \textbf{parallel} package, which is contained in the standard \textbf{R} image.
\textbf{SNOW} provides support for distributed computing over a simple network of compputers.
The general architecture of \textbf{SNOW} makes use of a master process that holds the data and launches the cluster, pushing the data to worker processes that operate upon it and return the results to the master.
\textbf{SNOW} makes use of several different communications mechanisms, including ssh and user-given sockets.
It's greatest shortcoming is the lack of persistent data, and it is not geared to very large datasets.

The two dominant external-based packages in \textbf{R} revolve around the \textbf{MPI} system, and \textbf{Spark}.

\textbf{pbdR} is a package making use of \textbf{MPI}.
The \textbf{pbdR} (programming with big data in R) project provides persistent data, with the \textbf{pbdDMAT} (programming with big data Distributed MATrices) package offering a user-friendly distributed matrix class to program with over a distributed system.
This abstraction breaks down with complexity, and a deep understanding of the powerful \textbf{MPI} system is eventually required, a task out of the league of most practicing statisticians.

\textbf{Spark} sees it's central interface in \textbf{R} through the \textbf{Sparklyr} package, which combines common \textbf{dplyr} methods with objects representing \textbf{Spark} dataframes.
Simple analyses are made very simple (assuming a well-configured and already running \textbf{Spark} instance), but custom iterative models are extremely difficult to create through the package in spite of \textbf{Spark's} support for it.

In the search for a distributed system for statistics, the world outside of R is not entirely barren.
The central issue with non-R distributed systems is that their focus is very obviously not statistics, and this shows in the level of support the platforms provide for statistical purposes.

\textbf{Hadoop} is a system predating and influencing \textbf{Spark}.
The project is a collection of utilities that facilitates cluster computing.
Jobs can be sent for parallel processing on the cluster directly to the utilities using .jar files, ``streamed'' using any executable file, or accessed through language-specific APIs.
Hadoop consists of a file-store component, known as Hadoop Distributed File System (HDFS), and a processing component, known as MapReduce.
The processing model is powerful, though incapable of the rapid iteration required in complex models.
However, the filesystem is widely used and provides an effective interface to large datasets.

In \textbf{Python}, the closest match to a high-level distributed system that could have statistical application is given by the python library \textbf{dask}\cite{rocklin2015dask}.
\textbf{dask} offers dynamic task scheduling through a central task graph, as well as a set of classes that encapsulate standard data manipulation structures such as NumPy arrays and Pandas dataframes.
The main difference is that the \textbf{dask} classes take advantage of the task scheduling, including online persistence across multiple nodes.
\textbf{dask} is a large and mature library, catering to many use-cases, and exists largely in the Pythonic ``Machine Learning'' culture in comparison to the R ``Statistics'' culture.
Accordingly, the focus is more tuned to the Python software developer putting existing ML models into a large-scale capacity.
Of all the distributed systems assessed so far, \textbf{dask} comes the closest to what an ideal platform would look like for a statistician, but it misses out on the statistical ecosystem of R, provides only a few select classes, and is tied entirely to the structure of the task graph.

\subsection{Evaluation}

The task of modelling over larger-than-memory data has a rich history of attempts to provide a solution.
For the requirements listed in section \ref{intro}, none of these come close to providing a satisfactory solution, though ironically the python library \textbf{dask} may come the closest.
The approaches put forward often do solve problems as defined otherwise, but all are either too cumbersome, incapable of handling large datasets, non-persistent, non-interactive, outside of the statistical ecosystem, excessively tied to their architecture, or all of the above.
The \textbf{largeScaleR} package is already performing better than some of the surveyed approaches, and section \ref{curr} will outline what has already been demonstrated in the prototype.

