The objective of this research project is to create a platfom for large-scale statistical computing, utilising the versatility and power of \texttt{R}.
Such a platform will allow statisticians to easily define and run complex distributed algorithms from within the \texttt{R} environment, rather than having to rely on external tools that never had statistical computation as a goal.
This platform will be demonstrated through the implementation of iterative models in \texttt{R}, and applying these models to real-world tasks on large-scale problems.

The following tasks will be undertaken as a part of the proposed research:

\begin{enumerate}
        \item Development of further proposed platform features, including:
                \begin{enumerate}
                        \item Fault tolerance.
                        \item Efficient memory usage through transient data and command chaining.
			\item Interfaces with external systems such as \texttt{Hadoop}.
                \end{enumerate}
        \item The demonstration of such a platform through implementing complex iterative statistical models on larger-than-memory datasets, including a Generalised Linear Model and variations thereof, as well as some multivariate techniques such as PCA or cluster analysis.
	\item Publishing the \textbf{largeScaleR} package on \texttt{CRAN}. This requires additional development including:
                \begin{enumerate}
                        \item Stability as may be expected from 90+\% test coverage.
                        \item Full documentation for all base functions.
                        \item A package vignette.
			\item Passing \texttt{CRAN} checks.
                \end{enumerate}
        \item Contributing to supporting packages.
        \item Publishing a technical report on the Stats blog, or arXiv.
	\item Publishing an article in an international, refereed journal, such as JSS, the R Journal, or similar.
        \item Presenting on the platform at useR!, or similar R developer conferences.
        \item Benchmarking a novel analysis on the platform with real-world data at a scale of 64+ nodes and 100+ GB data source size.
\end{enumerate}
