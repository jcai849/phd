Hadoop: The Definitive Guide
============================

1. Meet Hadoop
--------------

* Author argues that big data is relevant to everyone, affecting individuals
  as well as organisations; gives example of large amount of data generated
  through photos & recordings.
* The problem of big data is that storage capacities of hard drives have
  massively increased, but access speeds have not kept up.
* A potential solution is to read from multiple disks simultaneously, however
  there are drawbacks:
** Potential hardware failure
** Analysis requires combination of data between disks
* MapReduce is for batch processing, not suitable for interactive analysis
* Hadoop (being more than just MR) enables interactive analysis
* ``YARN is a cluster resource management system, which allows any distributed
    program (not just MapReduce) to run on data in a Hadoop cluster.''
* Comparison with multi-disk databases:
** B-trees as data structures form the backbone of most RDBMSs, is limited by
   seek time (seek time being the time to move the disk head to a location,
   paired with transfer rate, the disks bandwidth)
** MapReduce streams through a disk, thus limited by bandwidth
** Technologically, seek time improvement is slower than bandwidth improvement.
** Thus, RDBMSs are better for small updates, MapReduce better for full-data
   updates, and especially better for semi-structured or unstructured data
* Comparison with HPC:
** HPC approach is to distribute processing across a cluster, connected with 
   shared filesystem, using an API such as MPI for communication
** HPC works well for compute-bound jobs
** Hadoop offers data-locality, co-locating data with compute nodes, so data
   access is faster
** MPI in particular gives more programmer control, but this comes at the cost
   of manual data-flow manipulation, checkpoint, and recovery management
* History: Created by Doug Cutting, named after his child's toy elephant, used
  at Yahoo!, Facebook, etc. Broke world records in sorting big data

2. MapReduce
------------

* Running a MapReduce job with hadoop directly is a little unnerving;
  It is extremely verbose, and your eye picks up ``ERROR'' everywhere
** Mapper, Reducer, and the main classes are defined, then wrapped in .jar and
   run with hadoop. `HADOOP_CLASSPATH` should be set as the path of the .jar
* Single-machine processing limited, especially with resource management.
* ``MapReduce works by breaking the processing into two phases: the map phase
   and the reduce phase. Each phase has key-value pairs as input and output''
* Terminology:
Job:: The unit of work that the client want to be performed; it consists of the
      input data, the MapReduce program, and configuration information
Task:: Hadoop runs the job by dividing it into tasks, of which there are two
       types: map tasks and resuce tasks. The tasks are scheduled using YARN
       and run on nodes in the cluster. If a task fails, it will be
       automatically rescheduled to run on a different node.
Split:: Hadoop divides the input to a MapReduce job into fixed-size pieces
	called input splits, or just splits. Hadoop creates one map task for
	each split, which runs the user-defined map function for each record in
	the split
* Load balancing is better when splits are smaller, to allow faster machines to
  take more splits. However, if splits are too small, too much processing goes
  into the overhead of managing splits. The recommended split size tends to be
  the size of an HDFS block (128Mb by default).
Data locality optimisation:: Hadoop tries to run the map task on a node where
	the input data resides in HDFS, to not use cluster bandwidth.
* Map tasks write their output to the local disk, not to HDFS, as they are only
  meant to be temporary results
Combiner:: Combiner functions can be defined which run on the map output at the
	   location of the mapper, serving to reduce the information sent
	   across the cluster to reducers, saving bandwidth.
* Hadoop can make use of arbitrary programs to serve as mappers and reducers,
  through streaming to stdin and stdout of the programs; this allows arbitrary
  languages to run MapReduce.

3. The Hadoop Distributed Filesystem
------------------------------------

* The key to understanding HDFS is that it is designed around a write-once,
  read many times access pattern. Analyses are typically performed on whole or
  majority of a large dataset, so throughput is far more important than latency
  in seek time, and it optimises for the former at the expense of the latter.
  Examples of the expense include:
** No support for multiple writers
** No support for modifications at arbitrary file offsets (append-only writes)
* Hadoop blocks are a major point of difference between HDFS and a standard
  single-disk filesystem:
** They are very large, to  minimise seek cost
** files smaller than a block do not occupy the full block's worth of storage
** files can be larger than any disk; blocks do not have to be contiguous
** Each block is replicated (default 3 times) to protect against corruption
* A list of all file blocks can be given with the following command:
+
[source,sh]
--------------------------
hdfs fsck / -files -blocks
--------------------------

* Two types of nodes make up a cluster:
Namenode:: Master; manages filesystem namespace and maintains a record of
	   metadata and filesystem and datanodes
Datanode:: Slave; Store and retrieve the blocks, performing direct operations
           on them

* If the namenode disappeared in a setup like this, all data would be lost,
  as there are no other means of knowing what is stored where, and on whom
* Multiple namenodes were introduced in hadoop 2 as _HDFS Federation_, not
  necessarily to solve this problem, but to allow greater scaling; in this
  setup, namenodes manage portions of the filesystem at various mount points,
  though datanodes, while registered to specific namenodes, store blocks from
  more than just that namenode's block pool.
* Namenodes still remain a single point of failure, thus _HDFS High
  Availability_ was introduced as well, wherein a pair of namenodes read and
  write to shared memory.
* Caching is available and useful for frequently accessed files, in the
  datanode's memory, rather than the usual reads from disk. This is performed
  through the client adding a cache directive to a cache pool for the namenode
  to direct.

NOTE: Hadoop runs with security disabled by default; client's identities are
      not authenticated

 * Access to hadoop file systems is available through the URI scheme; HDFS can
   be accessed through many including HTTP, NFS, FUSE, etc.
 

.Setting up a Pseudo-Distributed Cluster
. A stable version of Hadoop to be downloaded (better to just use the binaries)
  Older versions are more stable, as well as better supported by spark and 3rd
  party documentation (you're going to need it)
. Set up environment variable HADOOP_HOME, and add the bin and sbin folders to
  PATH
. Configuration files are essential; 
.. Setup a configuration folder (or use the etc directory in HADOOP_HOME;
   external folder is better) and point environment variable HADOOP_CONF_DIR
   to the folder
.. Fill it with config files: I used the files from
   https://github.com/tomwhite/hadoop-book/tree/master/conf[the book's GitHub
   repo]. Amusingly, the ``definitive guide'' required more than what was 
   provided, including a `capacity-scheduler.xml` file, from
   http://svn.apache.org/viewvc/hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/resources/capacity-scheduler.xml?revision=1495684
.. Add localhost to the slave file for YARN to setup the ResourceManager
. Now the cluster can be started. Format the namenode with 
+
[source,sh]
--------------------
hdfs namenode -format
---------------------
. Start the HDFS, YARN, and MapReduce daemons (in that order) with
+
[source,sh]
----------------------------------------
start-dfs.sh
start-yarn.sh
mr-jobhistory-daemon start historyserver
----------------------------------------
+
The effects can be checked through several ways:

.. Checking the logs in the $HADOOP_HOME/logs directory
.. `jps` to see what Java processes are running
.. `hdfs getconf -namenodes` and co. for direct queries
.. Web interface with the namenode on port 50070
. If that's all working, make a user directory with
+
[source,sh]
-------------------------------
hadoop fs -mkdir -p /user/$USER
-------------------------------
+
the command has a few other standard unix navigation utilities. A `tail` but
no `head` though, due to the HDFS block structure!

. Copy the local files of interest over with
+
[source,sh]
--------------------------------------------------------------------------------
hadoop fs -copyFromLocal <local> hdfs://localhost/user/$USER/<intended-location>
--------------------------------------------------------------------------------
+
Worth noting that hdfs is the default, and relative paths can be given

. Run the MapReduce job in the standard manner on the HDFS files; see
  $HADOOP_HOME/log if it doesn't work

. Stop the daemons in reverse order with
+
[source,sh]
----------------------------------------
mr-jobhistory-daemon stop historyserver
stop-yarn.sh
stop-dfs.sh
----------------------------------------

4. YARN
-------

* Yet Another Resource Negotiator
* Provides APIs for requesting and working with cluster resources, and has
  multiple schemes to manage and allocate resources (CPU & Memory)
* Introduced in Hadoop 2, improving MapReduce, but allowing other distributed
  paradigms as well
* Applications typically run on top of YARN, which runs on top of HDFS (cf. fig
  4-1)
* Services given by two daemons:
+
Resource Manager:: Manages the use of cluster resources (only one per cluster)
Node Manager:: Launches and monitors containers (one on each node)
* Containers execute application-specific processes with constrained resources
* Application run process: Client asks resource manager to run an application
  master process, which the resource manager finds a node manager to launch in
  a container
* Spark makes use of running one application per session, reusing the same
  containers; MapReduce uses one application per job.
* YARN contrasts with MapReduce1 in that MapReduce1 has jobtracker and
  tasktracker daemons to handle scheduling
* YARN offers various modes of scheduling, including FIFO, DRF, etc.

5. Hadoop I/O
-------------

* HDFS checksums all data written to it, as well as verifying checksums when reading
* Corrupted blocks can be healed through copying from valid replica blocks
* Checksums can be directly accessed through:
+
[source,sh]
-------------------
hadoop fs -checksum
-------------------

* Compression is useful for obvious reasons, but the compression format should
  allow splitting (e.g. bzip2) in order to be useful to MapReduce
Codec:: The implementation of a (de)?compression algorithm
* Hadoop comes with several codec interfaces for various compression formats
Serialisation:: The process of turning structured objects into a byte stream
* Serialisation made use of in distributed systems for interprocess
  communication, and storage; formats with very different lifetimes and
  requirements of the serialisation process
* Hadoop uses it's own serialisation format; Writables, which has the
  limitation of being very Java-centric
* Avro is an alternative serialisation system designed to overcome the issues
  that Writables has

10. Setting Up a Hadoop Cluster
-------------------------------

Setup differs a fair bit from pseudodistributed setup (beyond the obvious
additional hosts)
* Cluster manager tools are recommnded (unsure if this is good advice; usually
  not). Category includes Cloudera Manager and Apache Ambari
* Hadoop runs on commodity hardware, but this is not the same as low end; low
  end being false economy from maintenance requirements. Typical node RAM 65Gb+
* No benefit to using RAID due to replication already occuring between nodes.
  RAID does find use in naamenode storage
* High Availability not covered in book
* Network topology specified through java interface; not relevant for same-rack
  setups
* set ENV vars and ssh as per pseudo distributed, on all nodes (use
  `ssh-agent`)
* Install hadoop on all nodes in network
* Good practice to create separate user accounts for each set of processes;
  `hdfs`, `hadoop`, `yarn`, etc.
* Recommended to unpack distribution contents to `/usr/local` or `/opt`.
  change the owner to `hadoop`, follow the pseudodistributed pattern for PATH
  etc.
* All nodes require appropriate configurations. If all nodes are equivalent,
  they can have the same configuration
* As in pseudodistributed, `slaves` holds all the slave node names. This file
  doesn't need to be distributed to the slaves
* `hadoop-env.sh` wasn't mentioned earlier, but it is sourced by hadoop and a
  good place to put ENV vars including JAVA_HOME. There exist equivalent files
  for yarn and mapreduce
* `start-dfs.sh` as `hdfs` user:
** Starts namenode
** Starts datanode on every slave listed in `slaves` file
** Starts secondary namenode
* `start-yarn.sh`:
** Starts resource manager on local machine
** Starts node manager on each slave
* Important configuration files given in table 10-1 (I found the ones on the
  official documentation to be better though)
* HADOOP_LOG_DIR can be set, recommended as `/var/log/hadoop`
* ``Hadoop has a bewilderin number of configuration properties''
* Hadoop relies on kerberos for security
* Benchmarking is useful to test if hadoop is working. It comes with several
  tests located in $HADOOP_HOME/share/hadoop/mapreduce/*tests.jar

11. Administering Hadoop
------------------------

* `fsck` checks the filesystem for file health
* Blocks are verified every three weeks
* Snapshots are possible on HDFS and are cheap too; simply a copy of the
  namenode filesystem data

17. Hive
--------

A framework for data warehousing built on top of hadoop. Not particularly
relevant right now

19. Spark
---------

* Doesn't need to be installed on all nodes; YARN distributed relevant JARs to
  all executors
* YARN client mode is required for programs that have any interactive
  component, which differs from YARN cluster mode in the location of executor
  launches

Further Reading
---------------
* Chansler et al., The Hadoop Distributed File System
* Murthy et al., Apache Hadoop YARN (Addison-Wesley, 2014)
* Sammer, Hadoop Operations (O'Reilly, 2012)
* Hammerbacher, Information Systems and the Rise of the Data Scientist
